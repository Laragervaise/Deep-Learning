{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook used the course's material up to lecture 5\n",
    "\n",
    "##### Batch processing\n",
    "##### Data normalization\n",
    "##### Optimizers (SGD, \"vanilla\" SGD, Adam)\n",
    "##### Cross-entropy loss\n",
    "##### Softmax and Relu activation functions\n",
    "##### Two different architectures (one or two convolutional layers)\n",
    "##### Convolutions\n",
    "##### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables\n",
    "# Size of data\n",
    "size = 1000\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "# Number of hidden layers\n",
    "n_hidden_layers = 50\n",
    "# Criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Learning rate (step size)\n",
    "eta = 1e-1\n",
    "# Number of epochs\n",
    "nb_epochs = 500\n",
    "# Parameter for loss computation\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = \\\n",
    "prologue.generate_pair_sets(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one hot encode the class (not needed for now)\n",
    "def classes_to_one_hot(class_):\n",
    "    res = torch.zeros(10)#,dtype=int)\n",
    "    d1 = class_\n",
    "    res[d1.item()] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, n_hidden, two_conv_layers = False):\n",
    "        super(CNN, self).__init__()\n",
    "        self.two_conv_layers = two_conv_layers\n",
    "        \n",
    "        # Case 1 convolutional layer\n",
    "        if (not two_conv_layers):\n",
    "            \n",
    "            # Convolutional layer 1: 1*14*14 -> 10*12*12\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size = 3)\n",
    "            \n",
    "            # Fully connected layer 1: 360 (10*6*6) -> number of hidden nodes\n",
    "            self.fc1 = nn.Linear(360, n_hidden)\n",
    "            \n",
    "            # Fully connected layer 2: number of hidden nodes -> 10 outputs to recognize the digits\n",
    "            self.fc2 = nn.Linear(n_hidden, 10)\n",
    "            \n",
    "            # Last fully connected layer, which predicts if the first number is smaller or equal to the second\n",
    "            self.fc3 = nn.Linear(20,2)\n",
    "        \n",
    "        # Case 2 convolutional layers\n",
    "        else :\n",
    "            \n",
    "            # Convolutional layer 1: 1*14*14 -> 10*12*12\n",
    "            self.conv1 = nn.Conv2d(1, 10, kernel_size = 3)\n",
    "            \n",
    "            # Convolutional layer 2: 10*6*6 -> 10*4*4\n",
    "            self.conv2 = nn.Conv2d(10, 50, kernel_size = 3)\n",
    "            \n",
    "            # Fully connected layer 1: 200 (50*2*2) -> number of hidden nodes\n",
    "            self.fc1 = nn.Linear(200, n_hidden)\n",
    "            \n",
    "            # Fully connected layer 2: number of hidden nodes -> 10 outputs to recognize the digits\n",
    "            self.fc2 = nn.Linear(n_hidden, 10)\n",
    "            \n",
    "            # Last fully connected layer, which predicts if the first number is smaller or equal to the second\n",
    "            self.fc3 = nn.Linear(20,2)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # We separate the two images to feed them through the network separately\n",
    "        \n",
    "        # For the network to train, it needs to learn :\n",
    "            # Whether each of the predicted digits are right\n",
    "            # Whether its predicted ordering is right\n",
    "            \n",
    "        # Thus, we need to return through this forward function \n",
    "            # the two digits prediction as well as the ordering prediction\n",
    "            \n",
    "        # Conventions :\n",
    "            # For the digit prediction, the index of the maximum response corresponds to the digit\n",
    "            # For the ordering prediction, if the first number is smaller or equal to the second, predict 1 (true)\n",
    "        \n",
    "        \n",
    "        # Case 1 hidden layer\n",
    "        if (not self.two_conv_layers):\n",
    "            \n",
    "            ### Forward-pass\n",
    "            \n",
    "            ## Image 1\n",
    "            # Max-pooling with kernel_size and stride of 2 : 12*12 -> 6*6 & Relu\n",
    "            x1 = F.relu(F.max_pool2d(self.conv1(x1), kernel_size=2, stride=2))\n",
    "            # Relu for non linearity\n",
    "            x1 = F.relu(self.fc1(x1.view(-1,360)))\n",
    "            # Softmax for the digit classification\n",
    "            x1 = F.softmax(self.fc2(x1))\n",
    "            \n",
    "            ## Image 2\n",
    "            # Max-pooling with kernel_size and stride of 2 : 12*12 -> 6*6 & Relu\n",
    "            x2 = F.relu(F.max_pool2d(self.conv1(x2), kernel_size=2, stride=2))\n",
    "            # Relu for non linearity\n",
    "            x2 = F.relu(self.fc1(x2.view(-1,360)))\n",
    "            # Softmax for the digit classification\n",
    "            x2 = F.softmax(self.fc2(x2))  \n",
    "\n",
    "            # Concat them before moving on to the last layer\n",
    "            x1x2 = torch.cat((x1,x2),1)                             \n",
    "            y = F.softmax(self.fc3(x1x2))\n",
    "        \n",
    "        #Case 2 hidden layers\n",
    "        else :\n",
    "\n",
    "            ### Forward-pass\n",
    "            \n",
    "            ## Image 1\n",
    "            # Max-pooling with kernel_size and stride of 2 : 12*12 -> 6*6 & Relu\n",
    "            x1 = F.relu(F.max_pool2d(self.conv1(x1), kernel_size=2, stride=2))\n",
    "            # Max-pooling with kernel_size and stride of 2 : 4*4 -> 2*2 & Relu\n",
    "            x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "            # Relu for non linearity\n",
    "            x1 = F.relu(self.fc1(x1.view(-1,200)))\n",
    "            # Softmax for the digit classification\n",
    "            x1 = F.softmax(self.fc2(x1))\n",
    "            \n",
    "            ## Image 2\n",
    "            # Max-pooling with kernel_size and stride of 2 : 12*12 -> 6*6 & Relu\n",
    "            x2 = F.relu(F.max_pool2d(self.conv1(x2), kernel_size=2, stride=2))\n",
    "            # Max-pooling with kernel_size and stride of 2 : 4*4 -> 2*2 & Relu\n",
    "            x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "            # Relu for non linearity\n",
    "            x2 = F.relu(self.fc1(x2.view(-1,200)))\n",
    "            # Softmax for the digit classification\n",
    "            x2 = F.softmax(self.fc2(x2))  \n",
    "\n",
    "            # Concat them before moving on to the last layer\n",
    "            x1x2 = torch.cat((x1,x2),1)                             \n",
    "            y = F.softmax(self.fc3(x1x2))\n",
    "        \n",
    "        return x1, x2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, lambda_, nb_epochs, optimizer):\n",
    "    # param : lamba_ is the coefficient used for giving more or less importance to the digit loss compared to the ordering loss\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            \n",
    "            # Data normalization\n",
    "            mu, std = train_input.mean(), train_input.std()\n",
    "            train_input.sub_(mu).div_(std)\n",
    "            \n",
    "            # Retrieving the corresponding train batch\n",
    "            batch_train = train_input.narrow(0,b,batch_size)\n",
    "            # Batch of pixels of the first images\n",
    "            train_1 = batch_train.narrow(1,0,1)    \n",
    "            # Batch of pixels of the second images\n",
    "            train_2 = batch_train.narrow(1,1,1)\n",
    "            \n",
    "            # Retrieving the corresponding class batch\n",
    "            batch_classes = train_classes.narrow(0,b,batch_size)\n",
    "            # Batch of the classes of the first images\n",
    "            classes_1 = batch_classes.narrow(1,0,1).flatten()\n",
    "            # Batch of the classes of the second images\n",
    "            classes_2 = batch_classes.narrow(1,1,1).flatten()\n",
    "                        \n",
    "            ### Predictions & Loss\n",
    "            d1,d2,pred = model(train_1, train_2)\n",
    "            # Compute the loss for the first digit prediction\n",
    "            d1_loss = criterion(d1, classes_1)\n",
    "            # Compute the loss for the second digit prediction\n",
    "            d2_loss = criterion(d2, classes_2)\n",
    "            # Compute the loss for the ordering prediction\n",
    "            pred_loss = criterion(pred, train_target.narrow(0,b,batch_size))\n",
    "            \n",
    "            # Reinitialize to 0 the gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss = (d1_loss + d2_loss)/2 + pred_loss*lambda_\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Display the loss\n",
    "        print(\"epoch =\", e, \", loss = \",loss.item())    \n",
    "        \n",
    "        \n",
    "def compute_nb_errors(model, test_input, test_target, test_classes):\n",
    "    nb_errors_img1 = 0\n",
    "    nb_errors_img2 = 0\n",
    "    nb_errors_pred = 0\n",
    "    data_size = 0\n",
    "    \n",
    "    for b in range(0, train_input.size(0), batch_size):\n",
    "\n",
    "        # Retrieving the corresponding test batch\n",
    "        batch_test = test_input.narrow(0,b,batch_size)\n",
    "        # Batch of pixels of the first images\n",
    "        test_1 = batch_test.narrow(1,0,1)     \n",
    "        # Batch of pixels of the second images\n",
    "        test_2 = batch_test.narrow(1,1,1)\n",
    "\n",
    "        # Retrieving the corresponding class batch\n",
    "        batch_classes = test_classes.narrow(0,b,batch_size)\n",
    "        # Batch of the classes of the first images\n",
    "        classes_1 = batch_classes.narrow(1,0,1).flatten()\n",
    "        # Batch of the classes of the second images\n",
    "        classes_2 = batch_classes.narrow(1,1,1).flatten()\n",
    "\n",
    "        # Prediction\n",
    "        d1,d2,pred = model(test_1, test_2)\n",
    "        \n",
    "        # Translate the predictions values to predicted classes\n",
    "        d1_classes = d1.max(1).indices\n",
    "        d2_classes = d2.max(1).indices\n",
    "        pred_classes = pred.max(1).indices\n",
    "        \n",
    "        # Compute the number of errors\n",
    "        for i in range(b, b + batch_size):\n",
    "            data_size += 1\n",
    "            if (d1_classes[i-b] != classes_1[i-b]):\n",
    "                nb_errors_img1 += 1\n",
    "            if (d2_classes[i-b] != classes_2[i-b]):\n",
    "                nb_errors_img2 += 1\n",
    "            if (pred_classes[i-b] != test_target[i]):\n",
    "                nb_errors_pred += 1\n",
    "        \n",
    "    print(\"Number of total errors on image 1 classification : \", nb_errors_img1)\n",
    "    print(\"Accuracy on image 1 classification : \", 100-nb_errors_img1/data_size*100, \"%\")\n",
    "    print(\"Number of total errors on image 2 classification : \", nb_errors_img2)\n",
    "    print(\"Accuracy on image 2 classification : \", 100-nb_errors_img2/data_size*100, \"%\")\n",
    "    print(\"Number of total errors on ordering prediction : \", nb_errors_pred)\n",
    "    print(\"Accuracy on ordering prediction : \", 100-nb_errors_pred/data_size*100, \"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/conda/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/dave/conda/lib/python3.7/site-packages/ipykernel_launcher.py:75: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/dave/conda/lib/python3.7/site-packages/ipykernel_launcher.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 , loss =  2.973578453063965\n",
      "epoch = 1 , loss =  2.8664662837982178\n",
      "epoch = 2 , loss =  2.5590877532958984\n",
      "epoch = 3 , loss =  2.4093241691589355\n",
      "epoch = 4 , loss =  2.3336148262023926\n",
      "epoch = 5 , loss =  2.307248830795288\n",
      "epoch = 6 , loss =  2.294355630874634\n",
      "epoch = 7 , loss =  2.290672779083252\n",
      "epoch = 8 , loss =  2.2490901947021484\n",
      "epoch = 9 , loss =  2.2316231727600098\n",
      "epoch = 10 , loss =  2.2420172691345215\n",
      "epoch = 11 , loss =  2.245359420776367\n",
      "epoch = 12 , loss =  2.2704901695251465\n",
      "epoch = 13 , loss =  2.215036153793335\n",
      "epoch = 14 , loss =  2.219799518585205\n",
      "epoch = 15 , loss =  2.207732677459717\n",
      "epoch = 16 , loss =  2.2058193683624268\n",
      "epoch = 17 , loss =  2.216407537460327\n",
      "epoch = 18 , loss =  2.1901345252990723\n",
      "epoch = 19 , loss =  2.1795897483825684\n",
      "epoch = 20 , loss =  2.1836776733398438\n",
      "epoch = 21 , loss =  2.199657917022705\n",
      "epoch = 22 , loss =  2.169644832611084\n",
      "epoch = 23 , loss =  2.189760208129883\n",
      "epoch = 24 , loss =  2.175579786300659\n",
      "epoch = 25 , loss =  2.1803503036499023\n",
      "epoch = 26 , loss =  2.176356315612793\n",
      "epoch = 27 , loss =  2.180238723754883\n",
      "epoch = 28 , loss =  2.1807804107666016\n",
      "epoch = 29 , loss =  2.1769959926605225\n",
      "epoch = 30 , loss =  2.164431571960449\n",
      "epoch = 31 , loss =  2.161534309387207\n",
      "epoch = 32 , loss =  2.1468570232391357\n",
      "epoch = 33 , loss =  2.1498148441314697\n",
      "epoch = 34 , loss =  2.147200345993042\n",
      "epoch = 35 , loss =  2.1394007205963135\n",
      "epoch = 36 , loss =  2.136422634124756\n",
      "epoch = 37 , loss =  2.144077777862549\n",
      "epoch = 38 , loss =  2.1162519454956055\n",
      "epoch = 39 , loss =  2.113868474960327\n",
      "epoch = 40 , loss =  2.118557929992676\n",
      "epoch = 41 , loss =  2.1122515201568604\n",
      "epoch = 42 , loss =  2.129934549331665\n",
      "epoch = 43 , loss =  2.1264991760253906\n",
      "epoch = 44 , loss =  2.1189136505126953\n",
      "epoch = 45 , loss =  2.119426727294922\n",
      "epoch = 46 , loss =  2.1360714435577393\n",
      "epoch = 47 , loss =  2.114928960800171\n",
      "epoch = 48 , loss =  2.1159486770629883\n",
      "epoch = 49 , loss =  2.1123862266540527\n",
      "epoch = 50 , loss =  2.096982479095459\n",
      "epoch = 51 , loss =  2.09971284866333\n",
      "epoch = 52 , loss =  2.094719409942627\n",
      "epoch = 53 , loss =  2.094496250152588\n",
      "epoch = 54 , loss =  2.0898282527923584\n",
      "epoch = 55 , loss =  2.089188814163208\n",
      "epoch = 56 , loss =  2.0880932807922363\n",
      "epoch = 57 , loss =  2.0860726833343506\n",
      "epoch = 58 , loss =  2.0854454040527344\n",
      "epoch = 59 , loss =  2.0846738815307617\n",
      "epoch = 60 , loss =  2.084223985671997\n",
      "epoch = 61 , loss =  2.083488702774048\n",
      "epoch = 62 , loss =  2.0830416679382324\n",
      "epoch = 63 , loss =  2.0825436115264893\n",
      "epoch = 64 , loss =  2.0820229053497314\n",
      "epoch = 65 , loss =  2.0816895961761475\n",
      "epoch = 66 , loss =  2.0811638832092285\n",
      "epoch = 67 , loss =  2.0808112621307373\n",
      "epoch = 68 , loss =  2.0805134773254395\n",
      "epoch = 69 , loss =  2.0800414085388184\n",
      "epoch = 70 , loss =  2.0795843601226807\n",
      "epoch = 71 , loss =  2.0792577266693115\n",
      "epoch = 72 , loss =  2.078799247741699\n",
      "epoch = 73 , loss =  2.078427314758301\n",
      "epoch = 74 , loss =  2.0781242847442627\n",
      "epoch = 75 , loss =  2.0776607990264893\n",
      "epoch = 76 , loss =  2.077353000640869\n",
      "epoch = 77 , loss =  2.0770156383514404\n",
      "epoch = 78 , loss =  2.0766539573669434\n",
      "epoch = 79 , loss =  2.0763933658599854\n",
      "epoch = 80 , loss =  2.076007604598999\n",
      "epoch = 81 , loss =  2.075700521469116\n",
      "epoch = 82 , loss =  2.0754871368408203\n",
      "epoch = 83 , loss =  2.075024366378784\n",
      "epoch = 84 , loss =  2.0746042728424072\n",
      "epoch = 85 , loss =  2.074293851852417\n",
      "epoch = 86 , loss =  2.073798418045044\n",
      "epoch = 87 , loss =  2.0734949111938477\n",
      "epoch = 88 , loss =  2.072946786880493\n",
      "epoch = 89 , loss =  2.0726656913757324\n",
      "epoch = 90 , loss =  2.0717387199401855\n",
      "epoch = 91 , loss =  2.0710370540618896\n",
      "epoch = 92 , loss =  2.0705082416534424\n",
      "epoch = 93 , loss =  2.0702128410339355\n",
      "epoch = 94 , loss =  2.0698633193969727\n",
      "epoch = 95 , loss =  2.069645643234253\n",
      "epoch = 96 , loss =  2.0693063735961914\n",
      "epoch = 97 , loss =  2.0691235065460205\n",
      "epoch = 98 , loss =  2.0688443183898926\n",
      "epoch = 99 , loss =  2.068615198135376\n",
      "epoch = 100 , loss =  2.068317413330078\n",
      "epoch = 101 , loss =  2.068099021911621\n",
      "epoch = 102 , loss =  2.0679214000701904\n",
      "epoch = 103 , loss =  2.0676162242889404\n",
      "epoch = 104 , loss =  2.0674571990966797\n",
      "epoch = 105 , loss =  2.067296028137207\n",
      "epoch = 106 , loss =  2.0669820308685303\n",
      "epoch = 107 , loss =  2.0668530464172363\n",
      "epoch = 108 , loss =  2.066689968109131\n",
      "epoch = 109 , loss =  2.066441535949707\n",
      "epoch = 110 , loss =  2.0662317276000977\n",
      "epoch = 111 , loss =  2.066131830215454\n",
      "epoch = 112 , loss =  2.065824031829834\n",
      "epoch = 113 , loss =  2.0656349658966064\n",
      "epoch = 114 , loss =  2.0654733180999756\n",
      "epoch = 115 , loss =  2.065293788909912\n",
      "epoch = 116 , loss =  2.065093994140625\n",
      "epoch = 117 , loss =  2.0647761821746826\n",
      "epoch = 118 , loss =  2.0644853115081787\n",
      "epoch = 119 , loss =  2.063870906829834\n",
      "epoch = 120 , loss =  2.062329053878784\n",
      "epoch = 121 , loss =  2.0587644577026367\n",
      "epoch = 122 , loss =  2.058720588684082\n",
      "epoch = 123 , loss =  2.058429479598999\n",
      "epoch = 124 , loss =  2.0582470893859863\n",
      "epoch = 125 , loss =  2.0578784942626953\n",
      "epoch = 126 , loss =  2.056635856628418\n",
      "epoch = 127 , loss =  2.0462324619293213\n",
      "epoch = 128 , loss =  2.0204458236694336\n",
      "epoch = 129 , loss =  1.9803965091705322\n",
      "epoch = 130 , loss =  1.9797561168670654\n",
      "epoch = 131 , loss =  1.97788667678833\n",
      "epoch = 132 , loss =  2.007842540740967\n",
      "epoch = 133 , loss =  1.9829522371292114\n",
      "epoch = 134 , loss =  1.9722261428833008\n",
      "epoch = 135 , loss =  1.9706709384918213\n",
      "epoch = 136 , loss =  1.9595547914505005\n",
      "epoch = 137 , loss =  1.9588614702224731\n",
      "epoch = 138 , loss =  1.9557225704193115\n",
      "epoch = 139 , loss =  1.9565187692642212\n",
      "epoch = 140 , loss =  1.951238751411438\n",
      "epoch = 141 , loss =  1.9530881643295288\n",
      "epoch = 142 , loss =  1.9552661180496216\n",
      "epoch = 143 , loss =  1.9508148431777954\n",
      "epoch = 144 , loss =  1.9479894638061523\n",
      "epoch = 145 , loss =  1.9459623098373413\n",
      "epoch = 146 , loss =  1.9453098773956299\n",
      "epoch = 147 , loss =  1.9445044994354248\n",
      "epoch = 148 , loss =  1.9439773559570312\n",
      "epoch = 149 , loss =  1.9434829950332642\n",
      "epoch = 150 , loss =  1.943169116973877\n",
      "epoch = 151 , loss =  1.9426242113113403\n",
      "epoch = 152 , loss =  1.9420188665390015\n",
      "epoch = 153 , loss =  1.9409899711608887\n",
      "epoch = 154 , loss =  1.9388178586959839\n",
      "epoch = 155 , loss =  1.9385439157485962\n",
      "epoch = 156 , loss =  1.9398577213287354\n",
      "epoch = 157 , loss =  1.9385950565338135\n",
      "epoch = 158 , loss =  1.9381099939346313\n",
      "epoch = 159 , loss =  1.9377003908157349\n",
      "epoch = 160 , loss =  1.937395453453064\n",
      "epoch = 161 , loss =  1.9373657703399658\n",
      "epoch = 162 , loss =  1.9368102550506592\n",
      "epoch = 163 , loss =  1.9365004301071167\n",
      "epoch = 164 , loss =  1.9361460208892822\n",
      "epoch = 165 , loss =  1.9359841346740723\n",
      "epoch = 166 , loss =  1.9371533393859863\n",
      "epoch = 167 , loss =  1.9363771677017212\n",
      "epoch = 168 , loss =  1.9358720779418945\n",
      "epoch = 169 , loss =  1.9357097148895264\n",
      "epoch = 170 , loss =  1.9351811408996582\n",
      "epoch = 171 , loss =  1.9349476099014282\n",
      "epoch = 172 , loss =  1.9345595836639404\n",
      "epoch = 173 , loss =  1.9343018531799316\n",
      "epoch = 174 , loss =  1.9340623617172241\n",
      "epoch = 175 , loss =  1.9339656829833984\n",
      "epoch = 176 , loss =  1.9337152242660522\n",
      "epoch = 177 , loss =  1.933518886566162\n",
      "epoch = 178 , loss =  1.934706211090088\n",
      "epoch = 179 , loss =  1.9339361190795898\n",
      "epoch = 180 , loss =  1.9336689710617065\n",
      "epoch = 181 , loss =  1.9333711862564087\n",
      "epoch = 182 , loss =  1.932331919670105\n",
      "epoch = 183 , loss =  1.9336026906967163\n",
      "epoch = 184 , loss =  1.9307833909988403\n",
      "epoch = 185 , loss =  1.9291255474090576\n",
      "epoch = 186 , loss =  1.9286515712738037\n",
      "epoch = 187 , loss =  1.9281519651412964\n",
      "epoch = 188 , loss =  1.93118155002594\n",
      "epoch = 189 , loss =  1.9285826683044434\n",
      "epoch = 190 , loss =  1.928320050239563\n",
      "epoch = 191 , loss =  1.9276371002197266\n",
      "epoch = 192 , loss =  1.927143931388855\n",
      "epoch = 193 , loss =  1.9271551370620728\n",
      "epoch = 194 , loss =  1.9274466037750244\n",
      "epoch = 195 , loss =  1.9266183376312256\n",
      "epoch = 196 , loss =  1.929357647895813\n",
      "epoch = 197 , loss =  1.9265388250350952\n",
      "epoch = 198 , loss =  1.9263684749603271\n",
      "epoch = 199 , loss =  1.926110029220581\n",
      "epoch = 200 , loss =  1.9262758493423462\n",
      "epoch = 201 , loss =  1.926663875579834\n",
      "epoch = 202 , loss =  1.9266892671585083\n",
      "epoch = 203 , loss =  1.9263607263565063\n",
      "epoch = 204 , loss =  1.9259510040283203\n",
      "epoch = 205 , loss =  1.9260683059692383\n",
      "epoch = 206 , loss =  1.9252440929412842\n",
      "epoch = 207 , loss =  1.9251736402511597\n",
      "epoch = 208 , loss =  1.9249768257141113\n",
      "epoch = 209 , loss =  1.9247339963912964\n",
      "epoch = 210 , loss =  1.9245905876159668\n",
      "epoch = 211 , loss =  1.9247348308563232\n",
      "epoch = 212 , loss =  1.924470067024231\n",
      "epoch = 213 , loss =  1.9260143041610718\n",
      "epoch = 214 , loss =  1.9248554706573486\n",
      "epoch = 215 , loss =  1.9248733520507812\n",
      "epoch = 216 , loss =  1.9246494770050049\n",
      "epoch = 217 , loss =  1.9241530895233154\n",
      "epoch = 218 , loss =  1.925750732421875\n",
      "epoch = 219 , loss =  1.924450397491455\n",
      "epoch = 220 , loss =  1.9243910312652588\n",
      "epoch = 221 , loss =  1.924174189567566\n",
      "epoch = 222 , loss =  1.9243288040161133\n",
      "epoch = 223 , loss =  1.9237134456634521\n",
      "epoch = 224 , loss =  1.9247394800186157\n",
      "epoch = 225 , loss =  1.9237899780273438\n",
      "epoch = 226 , loss =  1.923744559288025\n",
      "epoch = 227 , loss =  1.929219126701355\n",
      "epoch = 228 , loss =  1.9296071529388428\n",
      "epoch = 229 , loss =  1.9273630380630493\n",
      "epoch = 230 , loss =  1.9244427680969238\n",
      "epoch = 231 , loss =  1.9243271350860596\n",
      "epoch = 232 , loss =  1.9234793186187744\n",
      "epoch = 233 , loss =  1.9230774641036987\n",
      "epoch = 234 , loss =  1.9262733459472656\n",
      "epoch = 235 , loss =  1.9224494695663452\n",
      "epoch = 236 , loss =  1.9224909543991089\n",
      "epoch = 237 , loss =  1.9221465587615967\n",
      "epoch = 238 , loss =  1.9222983121871948\n",
      "epoch = 239 , loss =  1.92183518409729\n",
      "epoch = 240 , loss =  1.9244678020477295\n",
      "epoch = 241 , loss =  1.9222766160964966\n",
      "epoch = 242 , loss =  1.9223166704177856\n",
      "epoch = 243 , loss =  1.9220123291015625\n",
      "epoch = 244 , loss =  1.9218943119049072\n",
      "epoch = 245 , loss =  1.9217250347137451\n",
      "epoch = 246 , loss =  1.9211289882659912\n",
      "epoch = 247 , loss =  1.9212462902069092\n",
      "epoch = 248 , loss =  1.9223915338516235\n",
      "epoch = 249 , loss =  1.9213416576385498\n",
      "epoch = 250 , loss =  1.9218469858169556\n",
      "epoch = 251 , loss =  1.9212229251861572\n",
      "epoch = 252 , loss =  1.920602798461914\n",
      "epoch = 253 , loss =  1.920042634010315\n",
      "epoch = 254 , loss =  1.920017123222351\n",
      "epoch = 255 , loss =  1.9198236465454102\n",
      "epoch = 256 , loss =  1.919729232788086\n",
      "epoch = 257 , loss =  1.9199658632278442\n",
      "epoch = 258 , loss =  1.9199689626693726\n",
      "epoch = 259 , loss =  1.9201922416687012\n",
      "epoch = 260 , loss =  1.919379711151123\n",
      "epoch = 261 , loss =  1.9196194410324097\n",
      "epoch = 262 , loss =  1.919230341911316\n",
      "epoch = 263 , loss =  1.9192752838134766\n",
      "epoch = 264 , loss =  1.9192390441894531\n",
      "epoch = 265 , loss =  1.9191268682479858\n",
      "epoch = 266 , loss =  1.9189667701721191\n",
      "epoch = 267 , loss =  1.918757677078247\n",
      "epoch = 268 , loss =  1.918860912322998\n",
      "epoch = 269 , loss =  1.9192109107971191\n",
      "epoch = 270 , loss =  1.918747901916504\n",
      "epoch = 271 , loss =  1.918952465057373\n",
      "epoch = 272 , loss =  1.918833613395691\n",
      "epoch = 273 , loss =  1.9188964366912842\n",
      "epoch = 274 , loss =  1.9186968803405762\n",
      "epoch = 275 , loss =  1.9194430112838745\n",
      "epoch = 276 , loss =  1.919224739074707\n",
      "epoch = 277 , loss =  1.918671727180481\n",
      "epoch = 278 , loss =  1.9187036752700806\n",
      "epoch = 279 , loss =  1.9187278747558594\n",
      "epoch = 280 , loss =  1.9183077812194824\n",
      "epoch = 281 , loss =  1.9181647300720215\n",
      "epoch = 282 , loss =  1.9177380800247192\n",
      "epoch = 283 , loss =  1.917778730392456\n",
      "epoch = 284 , loss =  1.9181499481201172\n",
      "epoch = 285 , loss =  1.9185088872909546\n",
      "epoch = 286 , loss =  1.9176424741744995\n",
      "epoch = 287 , loss =  1.9178438186645508\n",
      "epoch = 288 , loss =  1.9172569513320923\n",
      "epoch = 289 , loss =  1.917914867401123\n",
      "epoch = 290 , loss =  1.9176888465881348\n",
      "epoch = 291 , loss =  1.9173266887664795\n",
      "epoch = 292 , loss =  1.9170584678649902\n",
      "epoch = 293 , loss =  1.9174129962921143\n",
      "epoch = 294 , loss =  1.9171006679534912\n",
      "epoch = 295 , loss =  1.9181455373764038\n",
      "epoch = 296 , loss =  1.9179152250289917\n",
      "epoch = 297 , loss =  1.9177651405334473\n",
      "epoch = 298 , loss =  1.9175517559051514\n",
      "epoch = 299 , loss =  1.9170995950698853\n",
      "epoch = 300 , loss =  1.9166631698608398\n",
      "epoch = 301 , loss =  1.9169113636016846\n",
      "epoch = 302 , loss =  1.917314052581787\n",
      "epoch = 303 , loss =  1.9169707298278809\n",
      "epoch = 304 , loss =  1.917415976524353\n",
      "epoch = 305 , loss =  1.9176349639892578\n",
      "epoch = 306 , loss =  1.9169347286224365\n",
      "epoch = 307 , loss =  1.9167276620864868\n",
      "epoch = 308 , loss =  1.916354775428772\n",
      "epoch = 309 , loss =  1.9164154529571533\n",
      "epoch = 310 , loss =  1.9162070751190186\n",
      "epoch = 311 , loss =  1.9165980815887451\n",
      "epoch = 312 , loss =  1.9170918464660645\n",
      "epoch = 313 , loss =  1.916985273361206\n",
      "epoch = 314 , loss =  1.9165176153182983\n",
      "epoch = 315 , loss =  1.9164130687713623\n",
      "epoch = 316 , loss =  1.916312575340271\n",
      "epoch = 317 , loss =  1.916321873664856\n",
      "epoch = 318 , loss =  1.9165607690811157\n",
      "epoch = 319 , loss =  1.9158631563186646\n",
      "epoch = 320 , loss =  1.9156272411346436\n",
      "epoch = 321 , loss =  1.9155302047729492\n",
      "epoch = 322 , loss =  1.9157525300979614\n",
      "epoch = 323 , loss =  1.9153172969818115\n",
      "epoch = 324 , loss =  1.915344476699829\n",
      "epoch = 325 , loss =  1.9152770042419434\n",
      "epoch = 326 , loss =  1.9189372062683105\n",
      "epoch = 327 , loss =  1.9159047603607178\n",
      "epoch = 328 , loss =  1.9164355993270874\n",
      "epoch = 329 , loss =  1.9157956838607788\n",
      "epoch = 330 , loss =  1.9165648221969604\n",
      "epoch = 331 , loss =  1.9162732362747192\n",
      "epoch = 332 , loss =  1.9161590337753296\n",
      "epoch = 333 , loss =  1.9157209396362305\n",
      "epoch = 334 , loss =  1.9154198169708252\n",
      "epoch = 335 , loss =  1.9153406620025635\n",
      "epoch = 336 , loss =  1.9185961484909058\n",
      "epoch = 337 , loss =  1.9152717590332031\n",
      "epoch = 338 , loss =  1.9153084754943848\n",
      "epoch = 339 , loss =  1.9149079322814941\n",
      "epoch = 340 , loss =  1.9148144721984863\n",
      "epoch = 341 , loss =  1.9151396751403809\n",
      "epoch = 342 , loss =  1.914594054222107\n",
      "epoch = 343 , loss =  1.9147343635559082\n",
      "epoch = 344 , loss =  1.9143428802490234\n",
      "epoch = 345 , loss =  1.914821743965149\n",
      "epoch = 346 , loss =  1.914729118347168\n",
      "epoch = 347 , loss =  1.9147738218307495\n",
      "epoch = 348 , loss =  1.9147244691848755\n",
      "epoch = 349 , loss =  1.9141478538513184\n",
      "epoch = 350 , loss =  1.9141099452972412\n",
      "epoch = 351 , loss =  1.91408109664917\n",
      "epoch = 352 , loss =  1.9141380786895752\n",
      "epoch = 353 , loss =  1.9140751361846924\n",
      "epoch = 354 , loss =  1.913923740386963\n",
      "epoch = 355 , loss =  1.9144667387008667\n",
      "epoch = 356 , loss =  1.914766550064087\n",
      "epoch = 357 , loss =  1.9143396615982056\n",
      "epoch = 358 , loss =  1.915426254272461\n",
      "epoch = 359 , loss =  1.9142967462539673\n",
      "epoch = 360 , loss =  1.9146679639816284\n",
      "epoch = 361 , loss =  1.914779782295227\n",
      "epoch = 362 , loss =  1.9145476818084717\n",
      "epoch = 363 , loss =  1.9140602350234985\n",
      "epoch = 364 , loss =  1.9141721725463867\n",
      "epoch = 365 , loss =  1.9144344329833984\n",
      "epoch = 366 , loss =  1.9139618873596191\n",
      "epoch = 367 , loss =  1.913874864578247\n",
      "epoch = 368 , loss =  1.9136769771575928\n",
      "epoch = 369 , loss =  1.9136574268341064\n",
      "epoch = 370 , loss =  1.9131195545196533\n",
      "epoch = 371 , loss =  1.913021445274353\n",
      "epoch = 372 , loss =  1.9133620262145996\n",
      "epoch = 373 , loss =  1.9136258363723755\n",
      "epoch = 374 , loss =  1.9132251739501953\n",
      "epoch = 375 , loss =  1.9131758213043213\n",
      "epoch = 376 , loss =  1.912895917892456\n",
      "epoch = 377 , loss =  1.9133422374725342\n",
      "epoch = 378 , loss =  1.912959337234497\n",
      "epoch = 379 , loss =  1.912736177444458\n",
      "epoch = 380 , loss =  1.9127018451690674\n",
      "epoch = 381 , loss =  1.9135273694992065\n",
      "epoch = 382 , loss =  1.9134376049041748\n",
      "epoch = 383 , loss =  1.9131635427474976\n",
      "epoch = 384 , loss =  1.9128155708312988\n",
      "epoch = 385 , loss =  1.912834644317627\n",
      "epoch = 386 , loss =  1.9130055904388428\n",
      "epoch = 387 , loss =  1.9127082824707031\n",
      "epoch = 388 , loss =  1.9131722450256348\n",
      "epoch = 389 , loss =  1.9125399589538574\n",
      "epoch = 390 , loss =  1.9126965999603271\n",
      "epoch = 391 , loss =  1.9132307767868042\n",
      "epoch = 392 , loss =  1.9134327173233032\n",
      "epoch = 393 , loss =  1.9131709337234497\n",
      "epoch = 394 , loss =  1.9130001068115234\n",
      "epoch = 395 , loss =  1.912701964378357\n",
      "epoch = 396 , loss =  1.9125903844833374\n",
      "epoch = 397 , loss =  1.913533091545105\n",
      "epoch = 398 , loss =  1.9129509925842285\n",
      "epoch = 399 , loss =  1.9127836227416992\n",
      "epoch = 400 , loss =  1.9125639200210571\n",
      "epoch = 401 , loss =  1.912839651107788\n",
      "epoch = 402 , loss =  1.9123477935791016\n",
      "epoch = 403 , loss =  1.912292242050171\n",
      "epoch = 404 , loss =  1.9119447469711304\n",
      "epoch = 405 , loss =  1.911913275718689\n",
      "epoch = 406 , loss =  1.911613941192627\n",
      "epoch = 407 , loss =  1.9119033813476562\n",
      "epoch = 408 , loss =  1.9152412414550781\n",
      "epoch = 409 , loss =  1.9125185012817383\n",
      "epoch = 410 , loss =  1.9122315645217896\n",
      "epoch = 411 , loss =  1.912003517150879\n",
      "epoch = 412 , loss =  1.9117294549942017\n",
      "epoch = 413 , loss =  1.9114265441894531\n",
      "epoch = 414 , loss =  1.9127105474472046\n",
      "epoch = 415 , loss =  1.9125232696533203\n",
      "epoch = 416 , loss =  1.9119476079940796\n",
      "epoch = 417 , loss =  1.912534236907959\n",
      "epoch = 418 , loss =  1.9122079610824585\n",
      "epoch = 419 , loss =  1.9130375385284424\n",
      "epoch = 420 , loss =  1.9124741554260254\n",
      "epoch = 421 , loss =  1.9125707149505615\n",
      "epoch = 422 , loss =  1.9121077060699463\n",
      "epoch = 423 , loss =  1.9115902185440063\n",
      "epoch = 424 , loss =  1.911267876625061\n",
      "epoch = 425 , loss =  1.9105565547943115\n",
      "epoch = 426 , loss =  1.900046467781067\n",
      "epoch = 427 , loss =  1.8770925998687744\n",
      "epoch = 428 , loss =  1.8931541442871094\n",
      "epoch = 429 , loss =  1.8773318529129028\n",
      "epoch = 430 , loss =  1.8798670768737793\n",
      "epoch = 431 , loss =  1.871153473854065\n",
      "epoch = 432 , loss =  1.8707870244979858\n",
      "epoch = 433 , loss =  1.8688945770263672\n",
      "epoch = 434 , loss =  1.8682663440704346\n",
      "epoch = 435 , loss =  1.8678491115570068\n",
      "epoch = 436 , loss =  1.8671876192092896\n",
      "epoch = 437 , loss =  1.8674098253250122\n",
      "epoch = 438 , loss =  1.8782696723937988\n",
      "epoch = 439 , loss =  1.879676103591919\n",
      "epoch = 440 , loss =  1.8671993017196655\n",
      "epoch = 441 , loss =  1.865976333618164\n",
      "epoch = 442 , loss =  1.8649529218673706\n",
      "epoch = 443 , loss =  1.8644542694091797\n",
      "epoch = 444 , loss =  1.8637253046035767\n",
      "epoch = 445 , loss =  1.862989902496338\n",
      "epoch = 446 , loss =  1.862309455871582\n",
      "epoch = 447 , loss =  1.8618088960647583\n",
      "epoch = 448 , loss =  1.861810326576233\n",
      "epoch = 449 , loss =  1.8609095811843872\n",
      "epoch = 450 , loss =  1.8605871200561523\n",
      "epoch = 451 , loss =  1.8597922325134277\n",
      "epoch = 452 , loss =  1.8518610000610352\n",
      "epoch = 453 , loss =  1.8604388236999512\n",
      "epoch = 454 , loss =  1.861554503440857\n",
      "epoch = 455 , loss =  1.8688955307006836\n",
      "epoch = 456 , loss =  1.860339879989624\n",
      "epoch = 457 , loss =  1.8631591796875\n",
      "epoch = 458 , loss =  1.858131766319275\n",
      "epoch = 459 , loss =  1.8576829433441162\n",
      "epoch = 460 , loss =  1.8579201698303223\n",
      "epoch = 461 , loss =  1.857689380645752\n",
      "epoch = 462 , loss =  1.8570579290390015\n",
      "epoch = 463 , loss =  1.850391149520874\n",
      "epoch = 464 , loss =  1.8496122360229492\n",
      "epoch = 465 , loss =  1.846737027168274\n",
      "epoch = 466 , loss =  1.8454569578170776\n",
      "epoch = 467 , loss =  1.8454493284225464\n",
      "epoch = 468 , loss =  1.845745325088501\n",
      "epoch = 469 , loss =  1.8448582887649536\n",
      "epoch = 470 , loss =  1.8452529907226562\n",
      "epoch = 471 , loss =  1.8448033332824707\n",
      "epoch = 472 , loss =  1.8447195291519165\n",
      "epoch = 473 , loss =  1.8450762033462524\n",
      "epoch = 474 , loss =  1.8443424701690674\n",
      "epoch = 475 , loss =  1.8446123600006104\n",
      "epoch = 476 , loss =  1.8444533348083496\n",
      "epoch = 477 , loss =  1.843886375427246\n",
      "epoch = 478 , loss =  1.8448665142059326\n",
      "epoch = 479 , loss =  1.8443635702133179\n",
      "epoch = 480 , loss =  1.8443056344985962\n",
      "epoch = 481 , loss =  1.8435533046722412\n",
      "epoch = 482 , loss =  1.8432426452636719\n",
      "epoch = 483 , loss =  1.8438609838485718\n",
      "epoch = 484 , loss =  1.8434014320373535\n",
      "epoch = 485 , loss =  1.8427460193634033\n",
      "epoch = 486 , loss =  1.8432419300079346\n",
      "epoch = 487 , loss =  1.8425346612930298\n",
      "epoch = 488 , loss =  1.8424155712127686\n",
      "epoch = 489 , loss =  1.8423798084259033\n",
      "epoch = 490 , loss =  1.8428683280944824\n",
      "epoch = 491 , loss =  1.8423436880111694\n",
      "epoch = 492 , loss =  1.843052864074707\n",
      "epoch = 493 , loss =  1.843324899673462\n",
      "epoch = 494 , loss =  1.8431934118270874\n",
      "epoch = 495 , loss =  1.8429539203643799\n",
      "epoch = 496 , loss =  1.8428771495819092\n",
      "epoch = 497 , loss =  1.8427681922912598\n",
      "epoch = 498 , loss =  1.8425949811935425\n",
      "epoch = 499 , loss =  1.8425580263137817\n",
      "Number of total errors on image 1 classification :  59\n",
      "Accuracy on image 1 classification :  94.1 %\n",
      "Number of total errors on image 2 classification :  77\n",
      "Accuracy on image 2 classification :  92.3 %\n",
      "Number of total errors on ordering prediction :  56\n",
      "Accuracy on ordering prediction :  94.4 %\n"
     ]
    }
   ],
   "source": [
    "###Tests\n",
    "\n",
    "# Model\n",
    "model = CNN(n_hidden_layers, two_conv_layers=False)\n",
    "\n",
    "### Optimizers\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = eta)\n",
    "optimizer = optim.SGD(model.parameters(), lr = eta, momentum = 0.9)\n",
    "\n",
    "# Train and error computation\n",
    "train_model(model, train_input, train_target, train_classes, lambda_, nb_epochs, optimizer)\n",
    "compute_nb_errors(model, test_input, test_target, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
