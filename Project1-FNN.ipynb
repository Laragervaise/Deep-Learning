{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook used the course's material up to lecture 5\n",
    "\n",
    "##### Batch processing\n",
    "##### Data normalization\n",
    "##### Optimizers (SGD, \"vanilla\" SGD, Adam)\n",
    "##### Cross-entropy loss\n",
    "##### Softmax and Relu activation functions\n",
    "##### Two different architectures (one or two hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables\n",
    "# Size of data\n",
    "size = 1000\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "# Number of hidden layers\n",
    "n_hidden_layers = 50\n",
    "# Criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Learning rate (step size)\n",
    "eta = 1e-1\n",
    "# Number of epochs\n",
    "nb_epochs = 500\n",
    "# Parameter for loss computation\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = \\\n",
    "prologue.generate_pair_sets(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one hot encode the class (not needed for now)\n",
    "def classes_to_one_hot(class_):\n",
    "    res = torch.zeros(10)#,dtype=int)\n",
    "    d1 = class_\n",
    "    res[d1.item()] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network\n",
    "class FNN(torch.nn.Module):\n",
    "    def __init__(self, n_hidden, two_hidden_layers = False):\n",
    "        super(FNN, self).__init__()\n",
    "        self.two_hidden_layers = two_hidden_layers\n",
    "        \n",
    "        # Case 1 hidden layer\n",
    "        if (not two_hidden_layers):\n",
    "    \n",
    "            # Fully connected layer 1: 196 (14*14) input pixels for each image -> number of hidden nodes\n",
    "            self.fc1 = nn.Linear(196, n_hidden)\n",
    "\n",
    "            # Fully connected layer 2: number of hidden nodes -> 10 outputs to recognize the digits\n",
    "            self.fc2 = nn.Linear(n_hidden, 10)\n",
    "\n",
    "            # Last fully connected layer, which predicts if the first number is smaller or equal to the second\n",
    "            self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "        # Case 2 hidden layers\n",
    "        else :\n",
    "            \n",
    "            # Fully connected layer 1: 196 (14*14) input pixels for each image -> number of hidden nodes\n",
    "            self.fc1 = nn.Linear(196, n_hidden)\n",
    "\n",
    "            # Fully connected layer 2: number of hidden nodes -> number of hidden nodes\n",
    "            self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "            # Fully connected layer 3: number of hidden nodes -> 10 outputs to recognize the digits\n",
    "            self.fc3 = nn.Linear(n_hidden, 10)\n",
    "\n",
    "            # Last fully connected layer, which predicts if the first number is smaller or equal to the second\n",
    "            self.fc4 = nn.Linear(20, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # We separate the two images to feed them through the network separately\n",
    "        \n",
    "        # For the network to train, it needs to learn :\n",
    "            # Whether each of the predicted digits are right\n",
    "            # Whether its predicted ordering is right\n",
    "            \n",
    "        # Thus, we need to return through this forward function \n",
    "            # the two digits prediction as well as the ordering prediction\n",
    "            \n",
    "        # Conventions :\n",
    "            # For the digit prediction, the index of the maximum response corresponds to the digit\n",
    "            # For the ordering prediction, if the first number is smaller or equal to the second, predict 1 (true)\n",
    "        \n",
    "        \n",
    "        # Case 1 hidden layer\n",
    "        if (not self.two_hidden_layers):\n",
    "            \n",
    "            ### Forward-pass\n",
    "            \n",
    "            ## Image 1\n",
    "            # Relu for non linearity\n",
    "            x1 = F.relu(self.fc1(x1))        \n",
    "            # Softmax for the digit classification\n",
    "            x1 = F.softmax(self.fc2(x1))   \n",
    "            \n",
    "            ## Image 2\n",
    "            # Relu for non linearity\n",
    "            x2 = F.relu(self.fc1(x2)) \n",
    "            # Softmax for the digit classification\n",
    "            x2 = F.softmax(self.fc2(x2))            \n",
    "\n",
    "            # Concat them before moving on to the last layer\n",
    "            x1x2 = torch.cat((x1,x2),1)                             \n",
    "            y = F.softmax(self.fc3(x1x2))\n",
    "        \n",
    "        #Case 2 hidden layers\n",
    "        else :\n",
    "            \n",
    "            ### Forward-pass\n",
    "            \n",
    "            ## Image 1\n",
    "            # Relu for non linearity\n",
    "            x1 = F.relu(self.fc1(x1))        \n",
    "            x1 = F.relu(self.fc2(x1))   \n",
    "            # Softmax for the digit classification\n",
    "            x1 = F.softmax(self.fc3(x1))\n",
    "            \n",
    "            ## Image 2\n",
    "            # Relu for non linearity\n",
    "            x2 = F.relu(self.fc1(x2)) \n",
    "            x2 = F.relu(self.fc2(x2)) \n",
    "            # Softmax for the digit classification\n",
    "            x2 = F.softmax(self.fc3(x2))   \n",
    "\n",
    "            # Concat them before moving on to the last layer\n",
    "            x1x2 = torch.cat((x1,x2),1)                             \n",
    "            y = F.softmax(self.fc4(x1x2))\n",
    "        \n",
    "        return x1, x2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, lambda_, nb_epochs, optimizer):\n",
    "    # param : lamba_ is the coefficient used for giving more or less importance to the digit loss compared to the ordering loss\n",
    "    \n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            \n",
    "            # Data normalization\n",
    "            mu, std = train_input.mean(), train_input.std()\n",
    "            train_input.sub_(mu).div_(std)\n",
    "            \n",
    "            # Retrieving the corresponding train batch\n",
    "            batch_train = train_input.narrow(0,b,batch_size)\n",
    "            # Batch of pixels of the first images\n",
    "            train_1 = batch_train.narrow(1,0,1).view(batch_size,-1)         \n",
    "            # Batch of pixels of the second images\n",
    "            train_2 = batch_train.narrow(1,1,1).view(batch_size,-1)\n",
    "            \n",
    "            # Retrieving the corresponding class batch\n",
    "            batch_classes = train_classes.narrow(0,b,batch_size)\n",
    "            # Batch of the classes of the first images\n",
    "            classes_1 = batch_classes.narrow(1,0,1).flatten()\n",
    "            # Batch of the classes of the second images\n",
    "            classes_2 = batch_classes.narrow(1,1,1).flatten()\n",
    "            \n",
    "            ### Predictions & Loss\n",
    "            d1,d2,pred = model(train_1, train_2)\n",
    "            # Compute the loss for the first digit prediction\n",
    "            d1_loss = criterion(d1, classes_1)\n",
    "            # Compute the loss for the second digit prediction\n",
    "            d2_loss = criterion(d2, classes_2)\n",
    "            # Compute the loss for the ordering prediction\n",
    "            pred_loss = criterion(pred, train_target.narrow(0,b,batch_size))\n",
    "            \n",
    "            # Reinitialize to 0 the gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss = (d1_loss + d2_loss)/2 + pred_loss*lambda_\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Display the loss\n",
    "        print(\"epoch =\", e, \", loss = \",loss.item())    \n",
    "        \n",
    "        \n",
    "def compute_nb_errors(model, test_input, test_target, test_classes):\n",
    "    nb_errors_img1 = 0\n",
    "    nb_errors_img2 = 0\n",
    "    nb_errors_pred = 0\n",
    "    data_size = 0\n",
    "    \n",
    "    for b in range(0, train_input.size(0), batch_size):\n",
    "\n",
    "        # Retrieving the corresponding test batch\n",
    "        batch_test = test_input.narrow(0,b,batch_size)\n",
    "        # Batch of pixels of the first images\n",
    "        test_1 = batch_test.narrow(1,0,1).view(batch_size,-1)         \n",
    "        # Batch of pixels of the second images\n",
    "        test_2 = batch_test.narrow(1,1,1).view(batch_size,-1)\n",
    "\n",
    "        # Retrieving the corresponding class batch\n",
    "        batch_classes = test_classes.narrow(0,b,batch_size)\n",
    "        # Batch of the classes of the first images\n",
    "        classes_1 = batch_classes.narrow(1,0,1).flatten()\n",
    "        # Batch of the classes of the second images\n",
    "        classes_2 = batch_classes.narrow(1,1,1).flatten()\n",
    "\n",
    "        # Prediction\n",
    "        d1,d2,pred = model(test_1, test_2)\n",
    "        \n",
    "        # Translate the predictions values to predicted classes\n",
    "        d1_classes = d1.max(1).indices\n",
    "        d2_classes = d2.max(1).indices\n",
    "        pred_classes = pred.max(1).indices\n",
    "        \n",
    "        # Compute the number of errors\n",
    "        for i in range(b, b + batch_size):\n",
    "            data_size += 1\n",
    "            if (d1_classes[i-b] != classes_1[i-b]):\n",
    "                nb_errors_img1 += 1\n",
    "            if (d2_classes[i-b] != classes_2[i-b]):\n",
    "                nb_errors_img2 += 1\n",
    "            if (pred_classes[i-b] != test_target[i]):\n",
    "                nb_errors_pred += 1\n",
    "        \n",
    "    print(\"Number of total errors on image 1 classification : \", nb_errors_img1)\n",
    "    print(\"Accuracy on image 1 classification : \", 100-nb_errors_img1/data_size*100, \"%\")\n",
    "    print(\"Number of total errors on image 2 classification : \", nb_errors_img2)\n",
    "    print(\"Accuracy on image 2 classification : \", 100-nb_errors_img2/data_size*100, \"%\")\n",
    "    print(\"Number of total errors on ordering prediction : \", nb_errors_pred)\n",
    "    print(\"Accuracy on ordering prediction : \", 100-nb_errors_pred/data_size*100, \"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/conda/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/dave/conda/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/dave/conda/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 , loss =  2.979917526245117\n",
      "epoch = 1 , loss =  2.885608434677124\n",
      "epoch = 2 , loss =  2.6408355236053467\n",
      "epoch = 3 , loss =  2.5096065998077393\n",
      "epoch = 4 , loss =  2.4266695976257324\n",
      "epoch = 5 , loss =  2.3399364948272705\n",
      "epoch = 6 , loss =  2.2785463333129883\n",
      "epoch = 7 , loss =  2.2419209480285645\n",
      "epoch = 8 , loss =  2.215785026550293\n",
      "epoch = 9 , loss =  2.1931910514831543\n",
      "epoch = 10 , loss =  2.1758363246917725\n",
      "epoch = 11 , loss =  2.1555418968200684\n",
      "epoch = 12 , loss =  2.1368677616119385\n",
      "epoch = 13 , loss =  2.1256942749023438\n",
      "epoch = 14 , loss =  2.1146786212921143\n",
      "epoch = 15 , loss =  2.1062557697296143\n",
      "epoch = 16 , loss =  2.1007440090179443\n",
      "epoch = 17 , loss =  2.10137939453125\n",
      "epoch = 18 , loss =  2.1059203147888184\n",
      "epoch = 19 , loss =  2.0989513397216797\n",
      "epoch = 20 , loss =  2.092463493347168\n",
      "epoch = 21 , loss =  2.0866806507110596\n",
      "epoch = 22 , loss =  2.0774424076080322\n",
      "epoch = 23 , loss =  2.081052541732788\n",
      "epoch = 24 , loss =  2.0794808864593506\n",
      "epoch = 25 , loss =  2.066382884979248\n",
      "epoch = 26 , loss =  2.0628862380981445\n",
      "epoch = 27 , loss =  2.0597639083862305\n",
      "epoch = 28 , loss =  2.0501227378845215\n",
      "epoch = 29 , loss =  2.0458688735961914\n",
      "epoch = 30 , loss =  2.041785955429077\n",
      "epoch = 31 , loss =  2.0347230434417725\n",
      "epoch = 32 , loss =  2.0325636863708496\n",
      "epoch = 33 , loss =  2.0045814514160156\n",
      "epoch = 34 , loss =  1.9990437030792236\n",
      "epoch = 35 , loss =  1.9826735258102417\n",
      "epoch = 36 , loss =  1.9699862003326416\n",
      "epoch = 37 , loss =  1.9564807415008545\n",
      "epoch = 38 , loss =  1.9478065967559814\n",
      "epoch = 39 , loss =  1.9430991411209106\n",
      "epoch = 40 , loss =  1.9391422271728516\n",
      "epoch = 41 , loss =  1.9358494281768799\n",
      "epoch = 42 , loss =  1.9325978755950928\n",
      "epoch = 43 , loss =  1.9294644594192505\n",
      "epoch = 44 , loss =  1.9263955354690552\n",
      "epoch = 45 , loss =  1.923965573310852\n",
      "epoch = 46 , loss =  1.9218621253967285\n",
      "epoch = 47 , loss =  1.9197214841842651\n",
      "epoch = 48 , loss =  1.9175444841384888\n",
      "epoch = 49 , loss =  1.915347695350647\n",
      "epoch = 50 , loss =  1.91303551197052\n",
      "epoch = 51 , loss =  1.910271167755127\n",
      "epoch = 52 , loss =  1.9075241088867188\n",
      "epoch = 53 , loss =  1.9053630828857422\n",
      "epoch = 54 , loss =  1.9039636850357056\n",
      "epoch = 55 , loss =  1.9025949239730835\n",
      "epoch = 56 , loss =  1.901681661605835\n",
      "epoch = 57 , loss =  1.9005699157714844\n",
      "epoch = 58 , loss =  1.8999587297439575\n",
      "epoch = 59 , loss =  1.8989487886428833\n",
      "epoch = 60 , loss =  1.8985663652420044\n",
      "epoch = 61 , loss =  1.8975306749343872\n",
      "epoch = 62 , loss =  1.8970146179199219\n",
      "epoch = 63 , loss =  1.8960180282592773\n",
      "epoch = 64 , loss =  1.8955707550048828\n",
      "epoch = 65 , loss =  1.8945034742355347\n",
      "epoch = 66 , loss =  1.8939120769500732\n",
      "epoch = 67 , loss =  1.8926582336425781\n",
      "epoch = 68 , loss =  1.891822099685669\n",
      "epoch = 69 , loss =  1.8906205892562866\n",
      "epoch = 70 , loss =  1.8900336027145386\n",
      "epoch = 71 , loss =  1.8891496658325195\n",
      "epoch = 72 , loss =  1.8890390396118164\n",
      "epoch = 73 , loss =  1.8876423835754395\n",
      "epoch = 74 , loss =  1.8874430656433105\n",
      "epoch = 75 , loss =  1.8865901231765747\n",
      "epoch = 76 , loss =  1.8863025903701782\n",
      "epoch = 77 , loss =  1.885764241218567\n",
      "epoch = 78 , loss =  1.8853708505630493\n",
      "epoch = 79 , loss =  1.8848628997802734\n",
      "epoch = 80 , loss =  1.8846561908721924\n",
      "epoch = 81 , loss =  1.8839714527130127\n",
      "epoch = 82 , loss =  1.8836792707443237\n",
      "epoch = 83 , loss =  1.8831050395965576\n",
      "epoch = 84 , loss =  1.8826158046722412\n",
      "epoch = 85 , loss =  1.8822269439697266\n",
      "epoch = 86 , loss =  1.8807311058044434\n",
      "epoch = 87 , loss =  1.8764214515686035\n",
      "epoch = 88 , loss =  1.8685250282287598\n",
      "epoch = 89 , loss =  1.8686552047729492\n",
      "epoch = 90 , loss =  1.8679580688476562\n",
      "epoch = 91 , loss =  1.8679413795471191\n",
      "epoch = 92 , loss =  1.867812156677246\n",
      "epoch = 93 , loss =  1.8670425415039062\n",
      "epoch = 94 , loss =  1.8675427436828613\n",
      "epoch = 95 , loss =  1.866845965385437\n",
      "epoch = 96 , loss =  1.8664984703063965\n",
      "epoch = 97 , loss =  1.8661777973175049\n",
      "epoch = 98 , loss =  1.865963101387024\n",
      "epoch = 99 , loss =  1.8661458492279053\n",
      "epoch = 100 , loss =  1.8654265403747559\n",
      "epoch = 101 , loss =  1.8653104305267334\n",
      "epoch = 102 , loss =  1.864965558052063\n",
      "epoch = 103 , loss =  1.8648399114608765\n",
      "epoch = 104 , loss =  1.8644287586212158\n",
      "epoch = 105 , loss =  1.8645031452178955\n",
      "epoch = 106 , loss =  1.8638689517974854\n",
      "epoch = 107 , loss =  1.863864779472351\n",
      "epoch = 108 , loss =  1.8634655475616455\n",
      "epoch = 109 , loss =  1.8634639978408813\n",
      "epoch = 110 , loss =  1.8630870580673218\n",
      "epoch = 111 , loss =  1.8628939390182495\n",
      "epoch = 112 , loss =  1.86271333694458\n",
      "epoch = 113 , loss =  1.8625227212905884\n",
      "epoch = 114 , loss =  1.862123727798462\n",
      "epoch = 115 , loss =  1.8623356819152832\n",
      "epoch = 116 , loss =  1.8617494106292725\n",
      "epoch = 117 , loss =  1.861710786819458\n",
      "epoch = 118 , loss =  1.8614041805267334\n",
      "epoch = 119 , loss =  1.861435055732727\n",
      "epoch = 120 , loss =  1.861205816268921\n",
      "epoch = 121 , loss =  1.8609639406204224\n",
      "epoch = 122 , loss =  1.8605551719665527\n",
      "epoch = 123 , loss =  1.8604851961135864\n",
      "epoch = 124 , loss =  1.8601272106170654\n",
      "epoch = 125 , loss =  1.8605502843856812\n",
      "epoch = 126 , loss =  1.8600540161132812\n",
      "epoch = 127 , loss =  1.8596460819244385\n",
      "epoch = 128 , loss =  1.8592122793197632\n",
      "epoch = 129 , loss =  1.8591840267181396\n",
      "epoch = 130 , loss =  1.8586777448654175\n",
      "epoch = 131 , loss =  1.858541488647461\n",
      "epoch = 132 , loss =  1.8579614162445068\n",
      "epoch = 133 , loss =  1.8572921752929688\n",
      "epoch = 134 , loss =  1.855175256729126\n",
      "epoch = 135 , loss =  1.8529629707336426\n",
      "epoch = 136 , loss =  1.8516545295715332\n",
      "epoch = 137 , loss =  1.8513396978378296\n",
      "epoch = 138 , loss =  1.850563406944275\n",
      "epoch = 139 , loss =  1.8506193161010742\n",
      "epoch = 140 , loss =  1.8499606847763062\n",
      "epoch = 141 , loss =  1.850020408630371\n",
      "epoch = 142 , loss =  1.849907398223877\n",
      "epoch = 143 , loss =  1.8495436906814575\n",
      "epoch = 144 , loss =  1.8492368459701538\n",
      "epoch = 145 , loss =  1.8489669561386108\n",
      "epoch = 146 , loss =  1.8486281633377075\n",
      "epoch = 147 , loss =  1.8485288619995117\n",
      "epoch = 148 , loss =  1.8484022617340088\n",
      "epoch = 149 , loss =  1.847921371459961\n",
      "epoch = 150 , loss =  1.8475971221923828\n",
      "epoch = 151 , loss =  1.8474633693695068\n",
      "epoch = 152 , loss =  1.847485899925232\n",
      "epoch = 153 , loss =  1.847282886505127\n",
      "epoch = 154 , loss =  1.8470451831817627\n",
      "epoch = 155 , loss =  1.8468722105026245\n",
      "epoch = 156 , loss =  1.8468775749206543\n",
      "epoch = 157 , loss =  1.8466858863830566\n",
      "epoch = 158 , loss =  1.846562385559082\n",
      "epoch = 159 , loss =  1.846288800239563\n",
      "epoch = 160 , loss =  1.8460854291915894\n",
      "epoch = 161 , loss =  1.8462659120559692\n",
      "epoch = 162 , loss =  1.8459374904632568\n",
      "epoch = 163 , loss =  1.8458855152130127\n",
      "epoch = 164 , loss =  1.8458752632141113\n",
      "epoch = 165 , loss =  1.8455301523208618\n",
      "epoch = 166 , loss =  1.845361351966858\n",
      "epoch = 167 , loss =  1.8453078269958496\n",
      "epoch = 168 , loss =  1.8452134132385254\n",
      "epoch = 169 , loss =  1.8452637195587158\n",
      "epoch = 170 , loss =  1.8450875282287598\n",
      "epoch = 171 , loss =  1.844835877418518\n",
      "epoch = 172 , loss =  1.8447034358978271\n",
      "epoch = 173 , loss =  1.844557762145996\n",
      "epoch = 174 , loss =  1.8444631099700928\n",
      "epoch = 175 , loss =  1.8444147109985352\n",
      "epoch = 176 , loss =  1.8446928262710571\n",
      "epoch = 177 , loss =  1.8442962169647217\n",
      "epoch = 178 , loss =  1.8440395593643188\n",
      "epoch = 179 , loss =  1.8438800573349\n",
      "epoch = 180 , loss =  1.8437891006469727\n",
      "epoch = 181 , loss =  1.8437728881835938\n",
      "epoch = 182 , loss =  1.84367835521698\n",
      "epoch = 183 , loss =  1.843437671661377\n",
      "epoch = 184 , loss =  1.8432774543762207\n",
      "epoch = 185 , loss =  1.8432836532592773\n",
      "epoch = 186 , loss =  1.8432316780090332\n",
      "epoch = 187 , loss =  1.8430650234222412\n",
      "epoch = 188 , loss =  1.8430571556091309\n",
      "epoch = 189 , loss =  1.8430745601654053\n",
      "epoch = 190 , loss =  1.8428417444229126\n",
      "epoch = 191 , loss =  1.842708706855774\n",
      "epoch = 192 , loss =  1.8426215648651123\n",
      "epoch = 193 , loss =  1.8424973487854004\n",
      "epoch = 194 , loss =  1.8424177169799805\n",
      "epoch = 195 , loss =  1.842355728149414\n",
      "epoch = 196 , loss =  1.8425030708312988\n",
      "epoch = 197 , loss =  1.8420941829681396\n",
      "epoch = 198 , loss =  1.8420522212982178\n",
      "epoch = 199 , loss =  1.8420770168304443\n",
      "epoch = 200 , loss =  1.8419420719146729\n",
      "epoch = 201 , loss =  1.8418550491333008\n",
      "epoch = 202 , loss =  1.8418983221054077\n",
      "epoch = 203 , loss =  1.8422929048538208\n",
      "epoch = 204 , loss =  1.8419265747070312\n",
      "epoch = 205 , loss =  1.8415753841400146\n",
      "epoch = 206 , loss =  1.8415381908416748\n",
      "epoch = 207 , loss =  1.8414812088012695\n",
      "epoch = 208 , loss =  1.8414242267608643\n",
      "epoch = 209 , loss =  1.8410688638687134\n",
      "epoch = 210 , loss =  1.8410367965698242\n",
      "epoch = 211 , loss =  1.840980052947998\n",
      "epoch = 212 , loss =  1.8408491611480713\n",
      "epoch = 213 , loss =  1.8408492803573608\n",
      "epoch = 214 , loss =  1.8407145738601685\n",
      "epoch = 215 , loss =  1.8404200077056885\n",
      "epoch = 216 , loss =  1.8403773307800293\n",
      "epoch = 217 , loss =  1.8406795263290405\n",
      "epoch = 218 , loss =  1.840537667274475\n",
      "epoch = 219 , loss =  1.840312123298645\n",
      "epoch = 220 , loss =  1.8403372764587402\n",
      "epoch = 221 , loss =  1.840052604675293\n",
      "epoch = 222 , loss =  1.8400448560714722\n",
      "epoch = 223 , loss =  1.8402154445648193\n",
      "epoch = 224 , loss =  1.840001106262207\n",
      "epoch = 225 , loss =  1.8396027088165283\n",
      "epoch = 226 , loss =  1.8395529985427856\n",
      "epoch = 227 , loss =  1.839598536491394\n",
      "epoch = 228 , loss =  1.8394665718078613\n",
      "epoch = 229 , loss =  1.8393335342407227\n",
      "epoch = 230 , loss =  1.839348554611206\n",
      "epoch = 231 , loss =  1.8395390510559082\n",
      "epoch = 232 , loss =  1.8393542766571045\n",
      "epoch = 233 , loss =  1.8391581773757935\n",
      "epoch = 234 , loss =  1.839064359664917\n",
      "epoch = 235 , loss =  1.838961124420166\n",
      "epoch = 236 , loss =  1.8389101028442383\n",
      "epoch = 237 , loss =  1.8387393951416016\n",
      "epoch = 238 , loss =  1.8387153148651123\n",
      "epoch = 239 , loss =  1.8387254476547241\n",
      "epoch = 240 , loss =  1.8392605781555176\n",
      "epoch = 241 , loss =  1.8391401767730713\n",
      "epoch = 242 , loss =  1.838658094406128\n",
      "epoch = 243 , loss =  1.8383214473724365\n",
      "epoch = 244 , loss =  1.8383312225341797\n",
      "epoch = 245 , loss =  1.8382658958435059\n",
      "epoch = 246 , loss =  1.8382104635238647\n",
      "epoch = 247 , loss =  1.838034987449646\n",
      "epoch = 248 , loss =  1.8380227088928223\n",
      "epoch = 249 , loss =  1.8379921913146973\n",
      "epoch = 250 , loss =  1.8379002809524536\n",
      "epoch = 251 , loss =  1.8378243446350098\n",
      "epoch = 252 , loss =  1.837770700454712\n",
      "epoch = 253 , loss =  1.837632417678833\n",
      "epoch = 254 , loss =  1.8375731706619263\n",
      "epoch = 255 , loss =  1.8374948501586914\n",
      "epoch = 256 , loss =  1.8374096155166626\n",
      "epoch = 257 , loss =  1.837339997291565\n",
      "epoch = 258 , loss =  1.837266445159912\n",
      "epoch = 259 , loss =  1.837151288986206\n",
      "epoch = 260 , loss =  1.8371334075927734\n",
      "epoch = 261 , loss =  1.8370965719223022\n",
      "epoch = 262 , loss =  1.8370494842529297\n",
      "epoch = 263 , loss =  1.836941123008728\n",
      "epoch = 264 , loss =  1.8370144367218018\n",
      "epoch = 265 , loss =  1.8368693590164185\n",
      "epoch = 266 , loss =  1.8367547988891602\n",
      "epoch = 267 , loss =  1.8367414474487305\n",
      "epoch = 268 , loss =  1.836655855178833\n",
      "epoch = 269 , loss =  1.8364746570587158\n",
      "epoch = 270 , loss =  1.8364161252975464\n",
      "epoch = 271 , loss =  1.8363486528396606\n",
      "epoch = 272 , loss =  1.8363094329833984\n",
      "epoch = 273 , loss =  1.8362663984298706\n",
      "epoch = 274 , loss =  1.8362387418746948\n",
      "epoch = 275 , loss =  1.8361375331878662\n",
      "epoch = 276 , loss =  1.836208701133728\n",
      "epoch = 277 , loss =  1.8360134363174438\n",
      "epoch = 278 , loss =  1.8361581563949585\n",
      "epoch = 279 , loss =  1.8362699747085571\n",
      "epoch = 280 , loss =  1.8360862731933594\n",
      "epoch = 281 , loss =  1.8357067108154297\n",
      "epoch = 282 , loss =  1.8356773853302002\n",
      "epoch = 283 , loss =  1.8357326984405518\n",
      "epoch = 284 , loss =  1.8356409072875977\n",
      "epoch = 285 , loss =  1.8354716300964355\n",
      "epoch = 286 , loss =  1.8357369899749756\n",
      "epoch = 287 , loss =  1.8355401754379272\n",
      "epoch = 288 , loss =  1.8354021310806274\n",
      "epoch = 289 , loss =  1.8353643417358398\n",
      "epoch = 290 , loss =  1.8353509902954102\n",
      "epoch = 291 , loss =  1.835257649421692\n",
      "epoch = 292 , loss =  1.8353819847106934\n",
      "epoch = 293 , loss =  1.8351414203643799\n",
      "epoch = 294 , loss =  1.8351225852966309\n",
      "epoch = 295 , loss =  1.8349391222000122\n",
      "epoch = 296 , loss =  1.8348978757858276\n",
      "epoch = 297 , loss =  1.8347930908203125\n",
      "epoch = 298 , loss =  1.8347820043563843\n",
      "epoch = 299 , loss =  1.834681510925293\n",
      "epoch = 300 , loss =  1.8346648216247559\n",
      "epoch = 301 , loss =  1.8345909118652344\n",
      "epoch = 302 , loss =  1.8345845937728882\n",
      "epoch = 303 , loss =  1.834518551826477\n",
      "epoch = 304 , loss =  1.834851861000061\n",
      "epoch = 305 , loss =  1.8345606327056885\n",
      "epoch = 306 , loss =  1.834464192390442\n",
      "epoch = 307 , loss =  1.8344674110412598\n",
      "epoch = 308 , loss =  1.8344577550888062\n",
      "epoch = 309 , loss =  1.834538459777832\n",
      "epoch = 310 , loss =  1.8344390392303467\n",
      "epoch = 311 , loss =  1.8340873718261719\n",
      "epoch = 312 , loss =  1.8340258598327637\n",
      "epoch = 313 , loss =  1.8340535163879395\n",
      "epoch = 314 , loss =  1.833988070487976\n",
      "epoch = 315 , loss =  1.8338561058044434\n",
      "epoch = 316 , loss =  1.8338987827301025\n",
      "epoch = 317 , loss =  1.8338137865066528\n",
      "epoch = 318 , loss =  1.8338192701339722\n",
      "epoch = 319 , loss =  1.8336776494979858\n",
      "epoch = 320 , loss =  1.8336923122406006\n",
      "epoch = 321 , loss =  1.8335967063903809\n",
      "epoch = 322 , loss =  1.8335881233215332\n",
      "epoch = 323 , loss =  1.8334946632385254\n",
      "epoch = 324 , loss =  1.833502173423767\n",
      "epoch = 325 , loss =  1.833435297012329\n",
      "epoch = 326 , loss =  1.8334767818450928\n",
      "epoch = 327 , loss =  1.8334643840789795\n",
      "epoch = 328 , loss =  1.8335084915161133\n",
      "epoch = 329 , loss =  1.8331894874572754\n",
      "epoch = 330 , loss =  1.8331859111785889\n",
      "epoch = 331 , loss =  1.8331705331802368\n",
      "epoch = 332 , loss =  1.833132266998291\n",
      "epoch = 333 , loss =  1.8329555988311768\n",
      "epoch = 334 , loss =  1.833014726638794\n",
      "epoch = 335 , loss =  1.8329440355300903\n",
      "epoch = 336 , loss =  1.8329408168792725\n",
      "epoch = 337 , loss =  1.8328107595443726\n",
      "epoch = 338 , loss =  1.8330622911453247\n",
      "epoch = 339 , loss =  1.8328286409378052\n",
      "epoch = 340 , loss =  1.832749843597412\n",
      "epoch = 341 , loss =  1.832709789276123\n",
      "epoch = 342 , loss =  1.8326901197433472\n",
      "epoch = 343 , loss =  1.8325691223144531\n",
      "epoch = 344 , loss =  1.8325865268707275\n",
      "epoch = 345 , loss =  1.832499384880066\n",
      "epoch = 346 , loss =  1.8325111865997314\n",
      "epoch = 347 , loss =  1.8324710130691528\n",
      "epoch = 348 , loss =  1.8324987888336182\n",
      "epoch = 349 , loss =  1.832349419593811\n",
      "epoch = 350 , loss =  1.832315444946289\n",
      "epoch = 351 , loss =  1.8321869373321533\n",
      "epoch = 352 , loss =  1.8321768045425415\n",
      "epoch = 353 , loss =  1.8320757150650024\n",
      "epoch = 354 , loss =  1.8320547342300415\n",
      "epoch = 355 , loss =  1.8319802284240723\n",
      "epoch = 356 , loss =  1.8319584131240845\n",
      "epoch = 357 , loss =  1.8319122791290283\n",
      "epoch = 358 , loss =  1.8319121599197388\n",
      "epoch = 359 , loss =  1.8318232297897339\n",
      "epoch = 360 , loss =  1.8318320512771606\n",
      "epoch = 361 , loss =  1.8317482471466064\n",
      "epoch = 362 , loss =  1.8317523002624512\n",
      "epoch = 363 , loss =  1.831655502319336\n",
      "epoch = 364 , loss =  1.831660270690918\n",
      "epoch = 365 , loss =  1.8315799236297607\n",
      "epoch = 366 , loss =  1.8315869569778442\n",
      "epoch = 367 , loss =  1.8315083980560303\n",
      "epoch = 368 , loss =  1.8315695524215698\n",
      "epoch = 369 , loss =  1.8314566612243652\n",
      "epoch = 370 , loss =  1.8314483165740967\n",
      "epoch = 371 , loss =  1.8314203023910522\n",
      "epoch = 372 , loss =  1.8314476013183594\n",
      "epoch = 373 , loss =  1.8314437866210938\n",
      "epoch = 374 , loss =  1.8314826488494873\n",
      "epoch = 375 , loss =  1.8311693668365479\n",
      "epoch = 376 , loss =  1.8311290740966797\n",
      "epoch = 377 , loss =  1.8311734199523926\n",
      "epoch = 378 , loss =  1.8311090469360352\n",
      "epoch = 379 , loss =  1.8309826850891113\n",
      "epoch = 380 , loss =  1.8314268589019775\n",
      "epoch = 381 , loss =  1.8311201333999634\n",
      "epoch = 382 , loss =  1.8309664726257324\n",
      "epoch = 383 , loss =  1.8309590816497803\n",
      "epoch = 384 , loss =  1.830925464630127\n",
      "epoch = 385 , loss =  1.8308188915252686\n",
      "epoch = 386 , loss =  1.8308346271514893\n",
      "epoch = 387 , loss =  1.8307626247406006\n",
      "epoch = 388 , loss =  1.830763816833496\n",
      "epoch = 389 , loss =  1.8306641578674316\n",
      "epoch = 390 , loss =  1.8306761980056763\n",
      "epoch = 391 , loss =  1.830599308013916\n",
      "epoch = 392 , loss =  1.8306100368499756\n",
      "epoch = 393 , loss =  1.830521583557129\n",
      "epoch = 394 , loss =  1.8305926322937012\n",
      "epoch = 395 , loss =  1.8305171728134155\n",
      "epoch = 396 , loss =  1.8305809497833252\n",
      "epoch = 397 , loss =  1.8304530382156372\n",
      "epoch = 398 , loss =  1.830474615097046\n",
      "epoch = 399 , loss =  1.8303637504577637\n",
      "epoch = 400 , loss =  1.830363154411316\n",
      "epoch = 401 , loss =  1.8302466869354248\n",
      "epoch = 402 , loss =  1.8302966356277466\n",
      "epoch = 403 , loss =  1.8302497863769531\n",
      "epoch = 404 , loss =  1.8303637504577637\n",
      "epoch = 405 , loss =  1.8301217555999756\n",
      "epoch = 406 , loss =  1.830041766166687\n",
      "epoch = 407 , loss =  1.8299314975738525\n",
      "epoch = 408 , loss =  1.8298753499984741\n",
      "epoch = 409 , loss =  1.8297970294952393\n",
      "epoch = 410 , loss =  1.829770803451538\n",
      "epoch = 411 , loss =  1.8297224044799805\n",
      "epoch = 412 , loss =  1.8297348022460938\n",
      "epoch = 413 , loss =  1.8297284841537476\n",
      "epoch = 414 , loss =  1.829703688621521\n",
      "epoch = 415 , loss =  1.8296420574188232\n",
      "epoch = 416 , loss =  1.829596996307373\n",
      "epoch = 417 , loss =  1.829559564590454\n",
      "epoch = 418 , loss =  1.8295271396636963\n",
      "epoch = 419 , loss =  1.8294543027877808\n",
      "epoch = 420 , loss =  1.8294494152069092\n",
      "epoch = 421 , loss =  1.829376220703125\n",
      "epoch = 422 , loss =  1.8293713331222534\n",
      "epoch = 423 , loss =  1.82929527759552\n",
      "epoch = 424 , loss =  1.8292919397354126\n",
      "epoch = 425 , loss =  1.8292217254638672\n",
      "epoch = 426 , loss =  1.8292162418365479\n",
      "epoch = 427 , loss =  1.8291449546813965\n",
      "epoch = 428 , loss =  1.8291387557983398\n",
      "epoch = 429 , loss =  1.8290719985961914\n",
      "epoch = 430 , loss =  1.8290648460388184\n",
      "epoch = 431 , loss =  1.8289982080459595\n",
      "epoch = 432 , loss =  1.8289929628372192\n",
      "epoch = 433 , loss =  1.8289254903793335\n",
      "epoch = 434 , loss =  1.8289272785186768\n",
      "epoch = 435 , loss =  1.8288570642471313\n",
      "epoch = 436 , loss =  1.8288686275482178\n",
      "epoch = 437 , loss =  1.8287975788116455\n",
      "epoch = 438 , loss =  1.828853964805603\n",
      "epoch = 439 , loss =  1.828826904296875\n",
      "epoch = 440 , loss =  1.8289920091629028\n",
      "epoch = 441 , loss =  1.8288835287094116\n",
      "epoch = 442 , loss =  1.8288623094558716\n",
      "epoch = 443 , loss =  1.8286114931106567\n",
      "epoch = 444 , loss =  1.828617811203003\n",
      "epoch = 445 , loss =  1.8285809755325317\n",
      "epoch = 446 , loss =  1.8285380601882935\n",
      "epoch = 447 , loss =  1.82844877243042\n",
      "epoch = 448 , loss =  1.8284856081008911\n",
      "epoch = 449 , loss =  1.8284854888916016\n",
      "epoch = 450 , loss =  1.8284821510314941\n",
      "epoch = 451 , loss =  1.828281283378601\n",
      "epoch = 452 , loss =  1.8282780647277832\n",
      "epoch = 453 , loss =  1.828269362449646\n",
      "epoch = 454 , loss =  1.8282232284545898\n",
      "epoch = 455 , loss =  1.8281277418136597\n",
      "epoch = 456 , loss =  1.828141450881958\n",
      "epoch = 457 , loss =  1.828096628189087\n",
      "epoch = 458 , loss =  1.828089714050293\n",
      "epoch = 459 , loss =  1.8280153274536133\n",
      "epoch = 460 , loss =  1.8280279636383057\n",
      "epoch = 461 , loss =  1.8279814720153809\n",
      "epoch = 462 , loss =  1.8280426263809204\n",
      "epoch = 463 , loss =  1.8281631469726562\n",
      "epoch = 464 , loss =  1.8281468152999878\n",
      "epoch = 465 , loss =  1.8278284072875977\n",
      "epoch = 466 , loss =  1.8278110027313232\n",
      "epoch = 467 , loss =  1.8278183937072754\n",
      "epoch = 468 , loss =  1.8277473449707031\n",
      "epoch = 469 , loss =  1.827650785446167\n",
      "epoch = 470 , loss =  1.8276596069335938\n",
      "epoch = 471 , loss =  1.8276221752166748\n",
      "epoch = 472 , loss =  1.8276145458221436\n",
      "epoch = 473 , loss =  1.8275322914123535\n",
      "epoch = 474 , loss =  1.8275446891784668\n",
      "epoch = 475 , loss =  1.8274798393249512\n",
      "epoch = 476 , loss =  1.8274813890457153\n",
      "epoch = 477 , loss =  1.8274078369140625\n",
      "epoch = 478 , loss =  1.8274154663085938\n",
      "epoch = 479 , loss =  1.827345848083496\n",
      "epoch = 480 , loss =  1.8273574113845825\n",
      "epoch = 481 , loss =  1.8272804021835327\n",
      "epoch = 482 , loss =  1.8272968530654907\n",
      "epoch = 483 , loss =  1.8272197246551514\n",
      "epoch = 484 , loss =  1.8272392749786377\n",
      "epoch = 485 , loss =  1.827258825302124\n",
      "epoch = 486 , loss =  1.8272743225097656\n",
      "epoch = 487 , loss =  1.8271279335021973\n",
      "epoch = 488 , loss =  1.8271201848983765\n",
      "epoch = 489 , loss =  1.8270875215530396\n",
      "epoch = 490 , loss =  1.8270753622055054\n",
      "epoch = 491 , loss =  1.826987385749817\n",
      "epoch = 492 , loss =  1.8269915580749512\n",
      "epoch = 493 , loss =  1.8269343376159668\n",
      "epoch = 494 , loss =  1.8269249200820923\n",
      "epoch = 495 , loss =  1.8268630504608154\n",
      "epoch = 496 , loss =  1.8268613815307617\n",
      "epoch = 497 , loss =  1.8268122673034668\n",
      "epoch = 498 , loss =  1.826810359954834\n",
      "epoch = 499 , loss =  1.826762318611145\n",
      "Number of total errors on image 1 classification :  98\n",
      "Accuracy on image 1 classification :  90.2 %\n",
      "Number of total errors on image 2 classification :  79\n",
      "Accuracy on image 2 classification :  92.1 %\n",
      "Number of total errors on ordering prediction :  51\n",
      "Accuracy on ordering prediction :  94.9 %\n"
     ]
    }
   ],
   "source": [
    "###Tests\n",
    "\n",
    "# Model\n",
    "model = FNN(n_hidden_layers, two_hidden_layers=False)\n",
    "\n",
    "### Optimizers\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr = eta)\n",
    "optimizer = optim.SGD(model.parameters(), lr = eta, momentum = 0.9)\n",
    "\n",
    "# Train and error computation\n",
    "train_model(model, train_input, train_target, train_classes, lambda_, nb_epochs, optimizer)\n",
    "compute_nb_errors(model, test_input, test_target, test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
